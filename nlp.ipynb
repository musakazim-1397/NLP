{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is Snoop Dogg\n"
     ]
    }
   ],
   "source": [
    "name='Snoop Dogg'\n",
    "print(f'my name is {name:{10}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my name is Snoop Dogg, I read in class                   10\n"
     ]
    }
   ],
   "source": [
    "class1= 10\n",
    "print(f'my name is {name:{10}}, I read in class {class1:{20}}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading from a pdf file using PyPDF2\n",
    "#first install PyPDF2 and then import it  --> import PyPDF2\n",
    "#open the pdf file with mode 'rb' eg: file= open('path',mode='rb')\n",
    "#read it using PyPDF2 --> pdf_reader= PyPDF2.PdfFileReader(file)\n",
    "#we've multiple methods available on pdf_reader object\n",
    "#eg: pdf_reader.getPage(), pdf_reader.numPages, pdf_reader.getPage(0).extractText()\n",
    "\n",
    "#to write to a pdf file we use: PdfFileWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(3, 8), match='phone'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#regular expression\n",
    "# \\d for digit, \\d{3} for 3 digits, \\d{3}-\\d{4} a combination of 3 digits and 4 digits\n",
    "#you don't need to install regular expressions, just import re\n",
    "import re\n",
    "text= 'my phone number is 408-999-1234'\n",
    "re.search('phone', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 8)\n"
     ]
    }
   ],
   "source": [
    "#to find out the span of all occurences \n",
    "for match in re.finditer('phone', text):\n",
    "    print(match.span())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<re.Match object; span=(19, 30), match='408-999-123'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#we use .search() or .finditer() for exact matches\n",
    "#if we don't know the exact match and just the pattern \n",
    "pattern = r'\\d{3}-\\d{3}-\\d{3}'\n",
    "re.search(pattern,text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "408-999-1234 408\n"
     ]
    }
   ],
   "source": [
    "#we also have quantifiers in reg expressions, which simply tell us how many times a pattern is repeating\n",
    "# '\\d{3} occuring 3 times\n",
    "# '\\d{3,5} occuring 3 to 5 times\n",
    "# '\\d{3,} occuring or more times\n",
    "# ? occurring once or more times\n",
    "\n",
    "\n",
    "#we also have grouping: which helps in indexing of the matched items\n",
    "pattern= r\"(\\d{3})-(\\d{3})-(\\d{4})\"\n",
    "my_match= re.search(pattern,text)\n",
    "print(my_match.group(), my_match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we also have logical AND, OR, and NOT operators in regular expressions\n",
    "re.search(r\"man|woman\", 'The man was here')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(0, 3), match='cat'>\n",
      "<re.Match object; span=(11, 14), match='hat'>\n",
      "<re.Match object; span=(15, 18), match='sat'>\n"
     ]
    }
   ],
   "source": [
    "#if you don't know the complete word then use wildcard (.)\n",
    "for match in re.finditer(r\".at\",\"cat in the hat sat\" ):\n",
    "    print(match)\n",
    "\n",
    "#the number of dots define the number of unkown characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#$ sign define ends with, and ^ define start with\n",
    "re.findall(r'\\d$', 'this ends with number 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.findall(r'^\\d', '1 is starting char in this string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['get rid of number ', ' in this string']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [^ ] is for exclusion, you can use it remove punctuations\n",
    "re.findall(r'[^\\d]+', 'get rid of number 34 in this string')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hypen-words', 'longish-words']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# [\\w] group of alphanumeric characters\n",
    "re.findall(r'[\\w]+-[\\w]+','find hypen-words, longish-words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla \t PROPN \t nsubj\n",
      "is \t AUX \t aux\n",
      "looking \t VERB \t ROOT\n",
      "to \t PART \t aux\n",
      "buy \t VERB \t xcomp\n",
      "U.S. \t PROPN \t nsubj\n",
      "start \t VERB \t ccomp\n",
      "up \t ADP \t prt\n",
      "for \t ADP \t prep\n",
      "$ \t SYM \t quantmod\n",
      "6 \t NUM \t compound\n",
      "million \t NUM \t pobj\n"
     ]
    }
   ],
   "source": [
    "#nlp basics\n",
    "#we will use spacy and nltk for NLP\n",
    "import spacy\n",
    "nlp= spacy.load('en_core_web_sm')\n",
    "doc= nlp(u'Tesla is looking to buy U.S. start up for $6 million')\n",
    "for token in doc:\n",
    "    print(token.text,'\\t',token.pos_,'\\t',token.dep_)\n",
    "    # print(token.pos,'   ',token.pos_)\n",
    "    # print(token.dep_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla PROPN NNP\n"
     ]
    }
   ],
   "source": [
    "#tokenization: first step in the NLP process, tokenization basically means the splitting up of text into component parts\n",
    "#we can also index the token through the doc object\n",
    "print(doc[0], doc[0].pos_, doc[0].tag_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla is looking to\n"
     ]
    }
   ],
   "source": [
    "#or select a range from the doc object\n",
    "print(doc[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc[1].is_stop\n",
    "#to check whether the token at that index is a stopword"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we\n",
      "'re\n",
      "here\n",
      "to\n",
      "help\n",
      "!\n",
      "Send\n",
      "snail\n",
      "-\n",
      "mail\n",
      ",\n",
      "email\n",
      "support@oursite.com\n",
      "or\n",
      "visit\n",
      "us\n",
      "at\n",
      "https://oursite.com\n"
     ]
    }
   ],
   "source": [
    "doc2=nlp(u\"we're here to help! Send snail-mail, email support@oursite.com or visit us at https://oursite.com\")\n",
    "for token in doc2:\n",
    "    print(token )\n",
    "\n",
    "#the email and the website address isn't divided"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    }
   ],
   "source": [
    "print(len(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#you can't assign anything to a doc object or the value of a token at an index\n",
    "#doc2[0]='hello' won't work\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S.\n",
      "$6 million\n"
     ]
    }
   ],
   "source": [
    "#named entities: spacy is smart enough to fathom if a token is a named entity\n",
    "for token in doc.ents:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Countries, cities, states\n",
      "Monetary values, including unit\n"
     ]
    }
   ],
   "source": [
    "for token in doc.ents:\n",
    "    print(spacy.explain(token.label_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tesla\n",
      "U.S.\n"
     ]
    }
   ],
   "source": [
    "#noun chunks:\n",
    "for token in doc.noun_chunks:\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
